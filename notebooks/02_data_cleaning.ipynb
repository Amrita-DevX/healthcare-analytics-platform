{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64625c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amrit\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Cleaning Environment Setup Complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "\"\"\"\n",
    "02_data_cleaning.ipynb - Data Cleaning\n",
    "\n",
    "Purpose:\n",
    "    Apply cleaning transformations based on EDA findings:\n",
    "    - Handle missing values\n",
    "    - Fix data types\n",
    "    - Remove duplicates\n",
    "    - Handle outliers\n",
    "    - Create clean dataset for feature engineering\n",
    "\n",
    "Based on insights from 01_eda_platform.ipynb\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "print(\"Data Cleaning Environment Setup Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33765e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "‚úì Beneficiaries: (116352, 32)\n",
      "‚úì Inpatient: (66773, 81)\n",
      "‚úì Outpatient: (790790, 76)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Config and Data\n",
    "\"\"\"\n",
    "Load configuration and raw data\n",
    "\"\"\"\n",
    "\n",
    "# Load config\n",
    "with open('../configs/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Define paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "INTERIM_PATH = PROJECT_ROOT / 'data' / 'interim'\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "beneficiaries = pd.read_parquet(INTERIM_PATH / 'beneficiaries.parquet')\n",
    "inpatient = pd.read_parquet(INTERIM_PATH / 'inpatient_claims.parquet')\n",
    "outpatient = pd.read_parquet(INTERIM_PATH / 'outpatient_claims.parquet')\n",
    "\n",
    "print(f\"‚úì Beneficiaries: {beneficiaries.shape}\")\n",
    "print(f\"‚úì Inpatient: {inpatient.shape}\")\n",
    "print(f\"‚úì Outpatient: {outpatient.shape}\")\n",
    "\n",
    "# Store original sizes\n",
    "original_sizes = {\n",
    "    'beneficiaries': len(beneficiaries),\n",
    "    'inpatient': len(inpatient),\n",
    "    'outpatient': len(outpatient)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c610c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HANDLING MISSING VALUES - BENEFICIARIES\n",
      "======================================================================\n",
      "\n",
      "Total missing values before: 110,891\n",
      "\n",
      "BENE_DEATH_DT: Missing means alive (no imputation needed)\n",
      "  Missing: 110,891\n",
      "\n",
      "Total missing values after: 110,891\n",
      "Reduction: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Handle Missing Values - Beneficiaries\n",
    "\"\"\"\n",
    "Strategy for missing values based on EDA findings\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HANDLING MISSING VALUES - BENEFICIARIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate missing before\n",
    "missing_before = beneficiaries.isnull().sum().sum()\n",
    "print(f\"\\nTotal missing values before: {missing_before:,}\")\n",
    "\n",
    "# Strategy 1: Death date missing = member is alive (not missing, it's intentional)\n",
    "if 'BENE_DEATH_DT' in beneficiaries.columns:\n",
    "    print(\"\\nBENE_DEATH_DT: Missing means alive (no imputation needed)\")\n",
    "    print(f\"  Missing: {beneficiaries['BENE_DEATH_DT'].isnull().sum():,}\")\n",
    "\n",
    "# Strategy 2: For chronic condition indicators, missing = 0 (no condition)\n",
    "chronic_cols = [col for col in beneficiaries.columns if 'CHRONIC' in col or '_SP_' in col]\n",
    "\n",
    "if chronic_cols:\n",
    "    print(f\"\\nChronic condition columns: {len(chronic_cols)}\")\n",
    "    print(\"  Strategy: Fill missing with 0 (no condition)\")\n",
    "    \n",
    "    for col in chronic_cols:\n",
    "        beneficiaries[col] = beneficiaries[col].fillna(0)\n",
    "    \n",
    "    print(\"  ‚úì Filled chronic condition missing values\")\n",
    "\n",
    "# Strategy 3: Drop rows with missing critical fields (if any)\n",
    "critical_fields = ['DESYNPUF_ID', 'BENE_BIRTH_DT']\n",
    "missing_critical = beneficiaries[critical_fields].isnull().any(axis=1).sum()\n",
    "\n",
    "if missing_critical > 0:\n",
    "    print(f\"\\nDropping {missing_critical} rows with missing critical fields\")\n",
    "    beneficiaries = beneficiaries.dropna(subset=critical_fields)\n",
    "\n",
    "# Calculate missing after\n",
    "missing_after = beneficiaries.isnull().sum().sum()\n",
    "print(f\"\\nTotal missing values after: {missing_after:,}\")\n",
    "print(f\"Reduction: {missing_before - missing_after:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a70149da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FIXING DATA TYPES\n",
      "======================================================================\n",
      "\n",
      "Converting BENE_BIRTH_DT to datetime...\n",
      " Converted. Missing after conversion: 0\n",
      "\n",
      "Converting BENE_DEATH_DT to datetime...\n",
      " Converted. Missing after conversion: 110891\n",
      "\n",
      "Inpatient Claims:\n",
      "  Converting CLM_FROM_DT...\n",
      "  Converting CLM_THRU_DT...\n",
      "  Converting CLM_ADMSN_DT...\n",
      "  Converting NCH_BENE_DSCHRG_DT...\n",
      "\n",
      "Outpatient Claims:\n",
      "  Converting CLM_FROM_DT...\n",
      "  Converting CLM_THRU_DT...\n",
      "\n",
      " Data type conversions complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Fix Data Types\n",
    "\"\"\"\n",
    "Convert columns to appropriate data types\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FIXING DATA TYPES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Date columns - convert to datetime\n",
    "date_columns = ['BENE_BIRTH_DT', 'BENE_DEATH_DT']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in beneficiaries.columns:\n",
    "        print(f\"\\nConverting {col} to datetime...\")\n",
    "        beneficiaries[col] = pd.to_datetime(\n",
    "            beneficiaries[col], \n",
    "            format='%Y%m%d', \n",
    "            errors='coerce'\n",
    "        )\n",
    "        print(f\" Converted. Missing after conversion: {beneficiaries[col].isnull().sum()}\")\n",
    "\n",
    "# Claims date columns\n",
    "claim_date_cols = ['CLM_FROM_DT', 'CLM_THRU_DT', 'CLM_ADMSN_DT', 'NCH_BENE_DSCHRG_DT']\n",
    "\n",
    "for df_name, df in [('Inpatient', inpatient), ('Outpatient', outpatient)]:\n",
    "    print(f\"\\n{df_name} Claims:\")\n",
    "    for col in claim_date_cols:\n",
    "        if col in df.columns:\n",
    "            print(f\"  Converting {col}...\")\n",
    "            df[col] = pd.to_datetime(df[col], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "print(\"\\n Data type conversions complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90a0ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HANDLING MISSING VALUES - BENEFICIARIES\n",
      "======================================================================\n",
      "\n",
      "Total missing values before: 110,891\n",
      "\n",
      "BENE_DEATH_DT: Missing means alive (no imputation needed)\n",
      "  Missing: 110,891\n",
      "\n",
      "Total missing values after: 110,891\n",
      "Reduction: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Handle Missing Values - Beneficiaries\n",
    "\"\"\"\n",
    "Strategy for missing values based on EDA findings\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HANDLING MISSING VALUES - BENEFICIARIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate missing before\n",
    "missing_before = beneficiaries.isnull().sum().sum()\n",
    "print(f\"\\nTotal missing values before: {missing_before:,}\")\n",
    "\n",
    "# Strategy 1: Death date missing = member is alive (not missing, it's intentional)\n",
    "if 'BENE_DEATH_DT' in beneficiaries.columns:\n",
    "    print(\"\\nBENE_DEATH_DT: Missing means alive (no imputation needed)\")\n",
    "    print(f\"  Missing: {beneficiaries['BENE_DEATH_DT'].isnull().sum():,}\")\n",
    "\n",
    "# Strategy 2: For chronic condition indicators, missing = 0 (no condition)\n",
    "chronic_cols = [col for col in beneficiaries.columns if 'CHRONIC' in col or '_SP_' in col]\n",
    "\n",
    "if chronic_cols:\n",
    "    print(f\"\\nChronic condition columns: {len(chronic_cols)}\")\n",
    "    print(\"  Strategy: Fill missing with 0 (no condition)\")\n",
    "    \n",
    "    for col in chronic_cols:\n",
    "        beneficiaries[col] = beneficiaries[col].fillna(0)\n",
    "    \n",
    "    print(\"  ‚úì Filled chronic condition missing values\")\n",
    "\n",
    "# Strategy 3: Drop rows with missing critical fields (if any)\n",
    "critical_fields = ['DESYNPUF_ID', 'BENE_BIRTH_DT']\n",
    "missing_critical = beneficiaries[critical_fields].isnull().any(axis=1).sum()\n",
    "\n",
    "if missing_critical > 0:\n",
    "    print(f\"\\nDropping {missing_critical} rows with missing critical fields\")\n",
    "    beneficiaries = beneficiaries.dropna(subset=critical_fields)\n",
    "\n",
    "# Calculate missing after\n",
    "missing_after = beneficiaries.isnull().sum().sum()\n",
    "print(f\"\\nTotal missing values after: {missing_after:,}\")\n",
    "print(f\"Reduction: {missing_before - missing_after:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ad8022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REMOVING DUPLICATES\n",
      "======================================================================\n",
      "\n",
      "Beneficiaries:\n",
      "  Duplicate member IDs: 0\n",
      "\n",
      "Inpatient Claims:\n",
      "  Duplicate claim IDs: 68\n",
      "  ‚úì Duplicates removed\n",
      "\n",
      "Outpatient Claims:\n",
      "  Duplicate claim IDs: 10975\n",
      "  ‚úì Duplicates removed\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Remove Duplicates\n",
    "\"\"\"\n",
    "Check for and remove duplicate records\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"REMOVING DUPLICATES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Beneficiaries - check for duplicate IDs\n",
    "print(\"\\nBeneficiaries:\")\n",
    "duplicates = beneficiaries['DESYNPUF_ID'].duplicated().sum()\n",
    "print(f\"  Duplicate member IDs: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"  Removing duplicates (keeping last)...\")\n",
    "    beneficiaries = beneficiaries.drop_duplicates(subset='DESYNPUF_ID', keep='last')\n",
    "    print(\"  ‚úì Duplicates removed\")\n",
    "\n",
    "# Claims - check for duplicate claim IDs\n",
    "print(\"\\nInpatient Claims:\")\n",
    "if 'CLM_ID' in inpatient.columns:\n",
    "    inp_dups = inpatient['CLM_ID'].duplicated().sum()\n",
    "    print(f\"  Duplicate claim IDs: {inp_dups}\")\n",
    "    \n",
    "    if inp_dups > 0:\n",
    "        inpatient = inpatient.drop_duplicates(subset='CLM_ID', keep='first')\n",
    "        print(\"  ‚úì Duplicates removed\")\n",
    "\n",
    "print(\"\\nOutpatient Claims:\")\n",
    "if 'CLM_ID' in outpatient.columns:\n",
    "    out_dups = outpatient['CLM_ID'].duplicated().sum()\n",
    "    print(f\"  Duplicate claim IDs: {out_dups}\")\n",
    "    \n",
    "    if out_dups > 0:\n",
    "        outpatient = outpatient.drop_duplicates(subset='CLM_ID', keep='first')\n",
    "        print(\"  ‚úì Duplicates removed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba0281ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HANDLING OUTLIERS\n",
      "======================================================================\n",
      "\n",
      "Inpatient Claims - Capping extreme costs at 99th percentile:\n",
      "\n",
      "Outpatient Claims - Capping extreme costs at 99th percentile:\n",
      "\n",
      "‚úì Outlier handling complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Handle Outliers\n",
    "\"\"\"\n",
    "Identify and handle extreme values in costs\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"HANDLING OUTLIERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find payment columns\n",
    "payment_cols_inp = [col for col in inpatient.columns if 'PMT' in col or 'PAYMENT' in col]\n",
    "payment_cols_out = [col for col in outpatient.columns if 'PMT' in col or 'PAYMENT' in col]\n",
    "\n",
    "def cap_outliers(df, columns, percentile=99):\n",
    "    \"\"\"\n",
    "    Cap extreme values at specified percentile\n",
    "    Strategy: Replace values above 99th percentile with 99th percentile value\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            threshold = df[col].quantile(percentile / 100)\n",
    "            outliers = (df[col] > threshold).sum()\n",
    "            \n",
    "            if outliers > 0:\n",
    "                print(f\"  {col}: Capping {outliers} values above ${threshold:.2f}\")\n",
    "                df[col] = df[col].clip(upper=threshold)\n",
    "    \n",
    "    return df\n",
    "\n",
    "if payment_cols_inp:\n",
    "    print(\"\\nInpatient Claims - Capping extreme costs at 99th percentile:\")\n",
    "    inpatient = cap_outliers(inpatient, payment_cols_inp, percentile=99)\n",
    "\n",
    "if payment_cols_out:\n",
    "    print(\"\\nOutpatient Claims - Capping extreme costs at 99th percentile:\")\n",
    "    outpatient = cap_outliers(outpatient, payment_cols_out, percentile=99)\n",
    "\n",
    "print(\"\\n‚úì Outlier handling complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d51679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA VALIDATION\n",
      "======================================================================\n",
      "\n",
      "1. Critical Field Validation:\n",
      "   DESYNPUF_ID: ‚úì PASS (0 nulls)\n",
      "\n",
      "2. Duplicate ID Check:\n",
      "   Beneficiaries: ‚úì PASS (0 duplicates)\n",
      "\n",
      "3. Data Volume Check:\n",
      "   Beneficiaries: ‚úì PASS (116,352 rows)\n",
      "   Inpatient: ‚úì PASS (66,705 rows)\n",
      "   Outpatient: ‚úì PASS (779,815 rows)\n",
      "\n",
      "4. Date Range Validation:\n",
      "   Birth dates: 1909-01-01 to 1983-12-01\n",
      "\n",
      "‚úì Validation complete\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Data Validation\n",
    "\"\"\"\n",
    "Validate cleaned data meets quality standards\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check 1: No null values in critical fields\n",
    "print(\"\\n1. Critical Field Validation:\")\n",
    "critical_fields = ['DESYNPUF_ID']\n",
    "\n",
    "for field in critical_fields:\n",
    "    null_count = beneficiaries[field].isnull().sum()\n",
    "    status = \"‚úì PASS\" if null_count == 0 else \"‚úó FAIL\"\n",
    "    print(f\"   {field}: {status} ({null_count} nulls)\")\n",
    "\n",
    "# Check 2: No duplicate IDs\n",
    "print(\"\\n2. Duplicate ID Check:\")\n",
    "dup_check = beneficiaries['DESYNPUF_ID'].duplicated().sum()\n",
    "status = \"‚úì PASS\" if dup_check == 0 else \"‚úó FAIL\"\n",
    "print(f\"   Beneficiaries: {status} ({dup_check} duplicates)\")\n",
    "\n",
    "# Check 3: Data volume check\n",
    "print(\"\\n3. Data Volume Check:\")\n",
    "min_rows = config['data_loading']['min_rows']\n",
    "\n",
    "for name, df in [('Beneficiaries', beneficiaries), ('Inpatient', inpatient), ('Outpatient', outpatient)]:\n",
    "    status = \"‚úì PASS\" if len(df) >= min_rows else \"‚úó FAIL\"\n",
    "    print(f\"   {name}: {status} ({len(df):,} rows)\")\n",
    "\n",
    "# Check 4: Date range validation\n",
    "print(\"\\n4. Date Range Validation:\")\n",
    "if 'BENE_BIRTH_DT' in beneficiaries.columns:\n",
    "    min_date = beneficiaries['BENE_BIRTH_DT'].min()\n",
    "    max_date = beneficiaries['BENE_BIRTH_DT'].max()\n",
    "    print(f\"   Birth dates: {min_date.date()} to {max_date.date()}\")\n",
    "\n",
    "print(\"\\n‚úì Validation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b91b6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SAVING CLEANED DATA\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 08:57:19 INFO mlflow.tracking.fluent: Experiment with name 'data_cleaning' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving cleaned data to interim/...\n",
      "‚úì Beneficiaries saved\n",
      "‚úì Inpatient claims saved\n",
      "‚úì Outpatient claims saved\n",
      "üèÉ View run data_cleaning_pipeline at: http://localhost:5000/#/experiments/242354557831061142/runs/d34991eda6ef48f785f1c31490a85c89\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/242354557831061142\n",
      "\n",
      "======================================================================\n",
      "DATA CLEANING COMPLETE\n",
      "======================================================================\n",
      "Beneficiaries: 116,352 (100.00% retained)\n",
      "Inpatient: 66,705 (99.90% retained)\n",
      "Outpatient: 779,815 (98.61% retained)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save Cleaned Data\n",
    "\"\"\"\n",
    "Save cleaned data back to interim/ folder (overwrite raw interim data)\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVING CLEANED DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save with MLflow tracking\n",
    "mlflow.set_tracking_uri(config['mlflow']['tracking_uri'])\n",
    "mlflow.set_experiment(\"data_cleaning\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"data_cleaning_pipeline\"):\n",
    "    \n",
    "    # Log original sizes\n",
    "    for key, value in original_sizes.items():\n",
    "        mlflow.log_param(f\"{key}_original\", value)\n",
    "    \n",
    "    # Log cleaned sizes\n",
    "    mlflow.log_param(\"beneficiaries_cleaned\", len(beneficiaries))\n",
    "    mlflow.log_param(\"inpatient_cleaned\", len(inpatient))\n",
    "    mlflow.log_param(\"outpatient_cleaned\", len(outpatient))\n",
    "    \n",
    "    # Calculate retention rates\n",
    "    ben_retention = len(beneficiaries) / original_sizes['beneficiaries'] * 100\n",
    "    inp_retention = len(inpatient) / original_sizes['inpatient'] * 100\n",
    "    out_retention = len(outpatient) / original_sizes['outpatient'] * 100\n",
    "    \n",
    "    mlflow.log_metric(\"beneficiaries_retention_pct\", ben_retention)\n",
    "    mlflow.log_metric(\"inpatient_retention_pct\", inp_retention)\n",
    "    mlflow.log_metric(\"outpatient_retention_pct\", out_retention)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    print(\"\\nSaving cleaned data to interim/...\")\n",
    "    beneficiaries.to_parquet(INTERIM_PATH / 'beneficiaries.parquet', index=False)\n",
    "    inpatient.to_parquet(INTERIM_PATH / 'inpatient_claims.parquet', index=False)\n",
    "    outpatient.to_parquet(INTERIM_PATH / 'outpatient_claims.parquet', index=False)\n",
    "    \n",
    "    print(\"‚úì Beneficiaries saved\")\n",
    "    print(\"‚úì Inpatient claims saved\")\n",
    "    print(\"‚úì Outpatient claims saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA CLEANING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Beneficiaries: {len(beneficiaries):,} ({ben_retention:.2f}% retained)\")\n",
    "print(f\"Inpatient: {len(inpatient):,} ({inp_retention:.2f}% retained)\")\n",
    "print(f\"Outpatient: {len(outpatient):,} ({out_retention:.2f}% retained)\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b30fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
